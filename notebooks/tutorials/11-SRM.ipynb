{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Response Modeling\n",
    "[Contributions](#contributions)\n",
    "\n",
    "What is common across different participants when performing the same task, for example watching a movie? In previous notebooks, inter-subject correlation and inter-subject functional correlation showed similarities in patterns of brain activity across subjects. We can exploit this similarity across subjects and find a common shared space where we can retain what is common across subjects. The shared response model (SRM) aims to achieve this common shared space. SRM is a functional alignment technique that maps participants into a shared, low-dimensionality feature space. Once SRM has been fit, other data from the same participants can be transformed into this space to be used as input to other analyses.\n",
    "\n",
    "The logic of SRM is as follows: The brain data for each participant is transformed into a n voxel by t time matrix. A certain number of k features are used to learn the mapping from voxel space into this shared space S. Every voxel in the brain loads on to these k features, which means a voxel has a weight w for the k features. w is first randomly initialized and then fit over a series of iterations to minimize the error in explaining the participant data. At the same time, the time course of these k features in the shared space is learned.\n",
    "\n",
    "If you are interested in more technical details, please refer to the [original paper](http://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model).\n",
    "\n",
    "Figure source [Cohen et al., 2017](https://www.nature.com/articles/nn.4499)![image](https://media.springernature.com/m685/nature-assets/neuro/journal/v20/n3/images/nn.4499-F4.jpg)\n",
    "\n",
    "SRM assumes that each participant was exposed to a common sequence of events or a shared stimulus. For this reason, movies or audioclips are typically used, although experiments (e.g. face/scene discrimination) where the trials are in the same order across participants have also been used with SRM. If counterbalancing was used between participants, it is technically possible to still perform SRM by rearranging the data, although mileage may vary. For best results, the units that are counterbalanced ought to be long (e.g. >30s). \n",
    "\n",
    "It is also important to note that SRM needs a lot of data to train on: up to 400 TRs are necessary to get stable performance. \n",
    "\n",
    "## Goal of this script\n",
    "    1. Learn to compute SRM.  \n",
    "    2. Use SRM to determine the exact time segment that a  movie clip correspond to.  \n",
    "    3. Use SRM to classify across multiple datasets. Here we will train on a movie              dataset and test on an image dataset.  \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    ">[1 Data File Preparation](#data_prep_srm)  \n",
    ">[2 SRM: Training data](#srm_training)  \n",
    ">[3 SRM: Testing data](#srm_testing)  \n",
    ">[4 Time segment matching](#time_seg)  \n",
    ">[5 Image class prediction](#image_pred)\n",
    "\n",
    "### Exercises\n",
    ">[1](#ex1)   [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)    [7](#ex7)    [8](#ex8)   \n",
    ">[Novel contribution](#novel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import scipy.spatial.distance as sp_distance\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "import brainiak.isfc\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import brainiak.funcalign.srm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%autosave 5\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data File Preparation <a id=\"data_prep_srm\"></a>\n",
    "\n",
    "We will work on the Raider dataset in this lab. It contains Ventral Temporal Cortex (VT) voxels of 10 subjects watching movie \"Raiders of the Lost Ark\" and 8 runs of 7 image categories. This dataset was first used in the following publication:\n",
    "Haxby, James V., J. Swaroop Guntupalli, Andrew C. Connolly, Yaroslav O. Halchenko, Bryan R. Conroy, M. Ida Gobbini, Michael Hanke, and Peter J. Ramadge. \"A common, high-dimensional model of the representational space in human ventral temporal cortex.\" Neuron 72, no. 2 (2011): 404-416. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up experiment metadata\n",
    "from utils import raider_data_dir\n",
    "print ('Dataset directory: %s' % raider_data_dir)\n",
    "\n",
    "# Where do you want to store the results\n",
    "dir_out = os.path.expanduser('~/srm_results')\n",
    "if not os.path.exists(dir_out):\n",
    "    os.makedirs(dir_out)\n",
    "    print('Dir %s created ' % dir_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and inspect the shape of bold data\n",
    "\n",
    "# load data\n",
    "movie_data = np.load(os.path.join(raider_data_dir, 'movie.npy'))\n",
    "\n",
    "# Pull out the shape data\n",
    "vox_num, nTR, num_subs = movie_data.shape  \n",
    "\n",
    "print('Participants ', num_subs)\n",
    "print('Voxels per participant ', vox_num)\n",
    "print('TRs per participant ', nTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the data into training and test sets. In this case we are going to split each participant's data down the middle, using the first half for training and the second half for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for sub in range(num_subs):\n",
    "    # Take the first half of TRs as training\n",
    "    # *The double dash means integer division\n",
    "    train_data.append(movie_data[:, :nTR//2, sub])  \n",
    "    # Take the second half of TRs as testing\n",
    "    test_data.append(movie_data[:, nTR//2:, sub])  \n",
    "del movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also normalize the data, with the training and testing data normalized independently. **This results in the test data being completely separate and uncontaminated by the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the subjects\n",
    "for sub in range(num_subs):    \n",
    "    # Do it for training data\n",
    "    train_data[sub] = stats.zscore(train_data[sub], axis=1, ddof=1)\n",
    "    train_data[sub] = np.nan_to_num(train_data[sub])\n",
    "    \n",
    "    # Do it for test data\n",
    "    test_data[sub] = stats.zscore(test_data[sub], axis=1, ddof=1)\n",
    "    test_data[sub] = np.nan_to_num(test_data[sub])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SRM: training data <a id=\"srm_training\"></a>\n",
    "\n",
    "We are now ready to fit the data with SRM. First we create an object in BrainIAK with a specified number of features and iterations. We then fit this to our data.\n",
    "\n",
    "Number of features should be chosen using cross-validation. In the SRM paper, 50 is chosen, so we use 50 here. Empirically, 10 or 20 iterations should be enough for the convergence of SRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 50  # How many features will you fit?\n",
    "n_iter = 20  # How many iterations of fitting will you perform\n",
    "\n",
    "# Create the SRM object\n",
    "srm = brainiak.funcalign.srm.SRM(n_iter=n_iter, features=features)\n",
    "\n",
    "# Fit the SRM data\n",
    "print('Fitting SRM, may take a minute ...')\n",
    "srm.fit(train_data)\n",
    "\n",
    "print('SRM has been fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the SRM model that was just created, we can explore the newly fitted data. Let's view the time course of the shared response captured by each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shared response\n",
    "print('SRM: Features X Time Points ', srm.s_.shape)\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.title('SRM: Features X Time Points')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('feature')\n",
    "plt.imshow(srm.s_, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Similarity of timepoints in shared space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this shared response to estimate how similar each timepoint is to one another. We can calculate the distance between all time points in this *k* dimensional space. If the points are close to one another then that indicates that they are similar. In the case below, blue means more similar. The yellow bands indicate time points where the patterns of activity diverge, potentially reflecting a unique time point in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = sp_distance.squareform(sp_distance.pdist(srm.s_.T))\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Distance between pairs of time points in shared space')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('TR')\n",
    "plt.imshow(dist_mat, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the shared response matrix, we now also have a $ voxels \\times features $ weight matrix for each participant that specifies how to map them into the shared space. This is essentially a functional alignment transformation matrix. Let's visualize the weights associated with each feature for a given voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('SRM: Weights x Features for One Voxel (One Subject)')\n",
    "srm.w_[0].shape\n",
    "plt.plot(srm.w_[0][1,:])\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('weight for one voxel')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The signal is denoised: reconstructing signal in subject space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrices are not only useful for projecting new data into a shared space but also for the reverse -- reconstructing data in subject space. This reconstructed activity is denoised because only the variance that was shared across participants is in the signal, reducing any within subject noise. We reconstruct the signal in subject space with the dot product of the weights for the first participant with the shared response feature time courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = srm.w_[0]  # Weights for subject 1\n",
    "S = srm.s_  # Shared response\n",
    "signal_srm0 = w0.dot(S)  # Reconstructed signal\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('SRM Reconstructed vs. Original Signal For One Voxel', fontsize=14)\n",
    "plt.plot(signal_srm0[100,:])\n",
    "plt.plot(train_data[0][100,:])\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('signal of one voxel')\n",
    "plt.legend(('Reconstructed Signal', 'Original Signal'), loc=(1.04,0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Projected signals have greater similarity in shared space than in subject space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transform (i.e., functionally align) individual participant data into the shared space. This means that each participant is represented as a $[k  \\times t] $ matrix based on what components of their brain activity was shared with other participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the SRM data into shared space\n",
    "shared_train = srm.transform(train_data)\n",
    "\n",
    "# Normalize data\n",
    "for subject in range(num_subs):\n",
    "    shared_train[subject] = stats.zscore(shared_train[subject], axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Perform ISC on projected signals in shared space\n",
    "\n",
    "Insofar as SRM worked, these feature time courses should be more similar across participants than the original voxel activity. We can test this with ISC: even though the shared features aren't in brain space, we can still use them as input to the ISC function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize the data back into an appropriate space for ISC\n",
    "raw_obj = np.zeros((train_data[0].shape[0], train_data[0].shape[1], len(train_data)))\n",
    "for ppt in range(len(train_data)):\n",
    "    raw_obj[:, :, ppt] = train_data[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants    \n",
    "corr_raw = brainiak.isfc.isc(raw_obj)\n",
    "corr_raw = np.nan_to_num(corr_raw)  \n",
    "\n",
    "# Reorganize the SRM transformed data back into an appropriate space for ISC\n",
    "shared_obj = np.zeros((shared_train[0].shape[0], shared_train[0].shape[1], len(train_data)))\n",
    "for ppt in range(len(train_data)):\n",
    "    shared_obj[:, :, ppt] = shared_train[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants        \n",
    "corr_shared = brainiak.isfc.isc(shared_obj)\n",
    "corr_shared = np.nan_to_num(corr_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Correlation between sub 0 and the group for all voxels')\n",
    "plt.hist(corr_raw);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of voxels')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Correlation between sub 0 and the group shared features')\n",
    "plt.hist(corr_shared);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of features')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "tstat = stats.ttest_ind(np.arctanh(corr_shared), np.arctanh(corr_raw))\n",
    "print('Independent samples t test between raw and SRM transformed data:', tstat.statistic, 'p:', tstat.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**<a id=\"ex1\"></a> Why does this work so well? What's wrong and how can you fix it?\n",
    "\n",
    "**A**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SRM: Test data <a id=\"srm_testing\"></a>\n",
    "\n",
    "We held out some data that were never seen by SRM and can use them to evaluate performance. If the dataset had other kinds of tasks (e.g., face vs. scene), we could similarly use the model to functionally align those data prior to performing other analyses (e.g., classification).\n",
    "\n",
    "The projected test data is computed by using the weight matrices for each subject, `S'=W^T*X'`, where `X'` is the testing data. The `srm.transform` method is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data into the shared space using the individual weight matrices\n",
    "shared_test = srm.transform(test_data)\n",
    "\n",
    "# Normalize data\n",
    "for subject in range(num_subs):\n",
    "    shared_test[subject] = stats.zscore(shared_test[subject], axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**<a id=\"ex2\"></a> Repeat the analysis above in the test set, comparing ISC in the original voxel space vs. the shared feature space. Interpret your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** <a id=\"ex3\"></a> In the analyses above we selected 50 features arbitrarily. This is an assumption about the dimensionality of the shared response. Try different *k* values and observe the SRM performance in the **test** set, defined as the statistical difference between raw and shared ISC. Try at least one lower and one higher value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time segment matching<a id=\"time_seg\"></a>\n",
    "One of the tests that has been used to quantify the benefits of SRM is *time segment matching*. This test involves taking isolated segments of fMRI data from a held-out subject and trying to determine *when in the movie* that segment occurred, by comparing the held-out segment to a whole-movie fMRI time series acquired from other subjects. So, for example, if you have a 30-second “mystery segment” of data from a held-out subject , you could compare it to all possible 30-second segments of data from the whole-movie time-series (acquired from other subjects) and see which one matches best. If the “mystery segment” matches best to the segment taken from 1:00 to 1:30 of the whole-movie time-series, that is your guess. If that guess matches the *actual* time period when the mystery segment was acquired, the guess is marked as being correct; otherwise, it is marked as being incorrect.  The whole process is shown in the figure below. The left figure shows how to learn W and test on the held-out subject and data in the time segment matching experiment. The projected test data is: `S'=W^T*X'`, where `X'` is the testing data. The right figure shows the *classifier* that is used to determine the best-matching segment (in this case, it is a max-correlation classifier). Note that the classifier computes the match to the correct segment and also all segments that *do not overlap in time* with the correct segment (segments that overlap with the correct segment are excluded, on the grounds that it might be difficult to discriminate between segments that closely overlap in time). The intuition behind time segment matching is that, if the movie fMRI time series is *well-aligned across participants and different parts of the movie have unique fMRI “signatures”* it should be possible to identify the “mystery segment” by comparing that segment to properly-timestamped fMRI timeseries from other participants. One might expect SRM to boost time-segment-matching accuracy by increasing the consistency of neural patterns across subjects. We will see below if this is the case. \n",
    "\n",
    "![image](imgs/lab11/srm_time_segment_matching.png)\n",
    "\n",
    "The following sequence of steps is used to perform time segment matching:\n",
    "\n",
    ">1. The dataset consists of all subjects watching the same movie.  \n",
    ">2. The dataset is split into a training and testing set.  \n",
    ">3. We fit SRM to the training data. The output is stored in `shared_train`. For comparison purposes, we can also use the raw data i.e. not fit by SRM.\n",
    ">4. Pass `shared_train` to the `time_segment_matching` function.\n",
    ">5. The `time_segment_matching` function performs the following steps:\n",
    ">> - It creates a sliding window of size `win_size`.\n",
    ">> - The time-series is broken into `nseg` overlapping windows. \n",
    ">> - Data is extracted for each of these segments.\n",
    ">> - One subject is taken out for leave-one-subject-out testing. Note the input data contains all subjects. The removal of one subject from a list for cross-validation, is similar to what we have covered in previous notebooks e.g., notebook-09-fcma.\n",
    ">> - A correlation is computed between the left-out subject and the average of the other subjects. This is similar to an ISC analysis. The `compute_correlation` function in BrainIAK is used to calculate this.  \n",
    ">> - For each segment, the maximum correlation value is found across subjects.  \n",
    ">>> - If the maximum correlation for a segment corresponds to the same segment in the test and training data, we score that as an accurate classification.  \n",
    ">> - The final accuracy of the classifier is returned for all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in a list of participants of voxel by TR data. Also specify how big the time segment is to be matched\n",
    "def time_segment_matching(data, win_size=10): \n",
    "    nsubjs = len(data)\n",
    "    (ndim, nsample) = data[0].shape\n",
    "    accu = np.zeros(shape=nsubjs)\n",
    "    nseg = nsample - win_size \n",
    "    # mysseg prediction prediction\n",
    "    trn_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    # the trn data also include the tst data, but will be subtracted when calculating A\n",
    "    for m in range(nsubjs):\n",
    "        for w in range(win_size):\n",
    "            trn_data[w*ndim:(w+1)*ndim,:] += data[m][:,w:(w+nseg)]\n",
    "    for tst_subj in range(nsubjs):\n",
    "        tst_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "        for w in range(win_size):\n",
    "            tst_data[w*ndim:(w+1)*ndim,:] = data[tst_subj][:,w:(w+nseg)]\n",
    "\n",
    "        A =  np.nan_to_num(stats.zscore((trn_data - tst_data),axis=0, ddof=1))\n",
    "        B =  np.nan_to_num(stats.zscore(tst_data,axis=0, ddof=1))\n",
    "\n",
    "        # compute correlation matrix\n",
    "        corr_mtx = compute_correlation(B.T,A.T)\n",
    "\n",
    "        # The correlation classifier.\n",
    "        for i in range(nseg):\n",
    "            for j in range(nseg):\n",
    "                # exclude segments overlapping with the testing segment\n",
    "                if abs(i-j)<win_size and i != j :\n",
    "                    corr_mtx[i,j] = -np.inf\n",
    "        max_idx =  np.argmax(corr_mtx, axis=1)\n",
    "        accu[tst_subj] = sum(max_idx == range(nseg)) / nseg\n",
    "\n",
    "        # Print accuracy\n",
    "        print(\"Accuracy for subj %d is: %0.4f\" % (tst_subj, accu[tst_subj] ))\n",
    "        \n",
    "    print(\"The average accuracy among all subjects is {0:f} +/- {1:f}\".format(np.mean(accu), np.std(accu)))\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for voxel data\n",
    "accu_train_r = time_segment_matching(train_data, win_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**<a id=\"ex4\"></a> The results above show that trying to determine a time segment from raw data is quite difficult. Note that 13% is not too bad since the chance accuracy is *1/number of segments = 1/1082= 0.09%*, but let's see if we can do better with SRM. Use SRM to transform the data and discover how it improves the identification of time segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:**<a id=\"ex5\"></a> Perform time segment matching on both raw and SRM transformed test data. Make a plot of the accuracies for 4 conditions: raw training data, transformed training data, raw testing data, transformed testing data. Plot the average with error bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> How does the number of subjects used to fit SRM impact the performance? Fit SRMs on different number of subjects and run time segment matching. Start from 2 subjects. Plot average accuracy across subjects vs. number of subjects used to fit SRM. For example, train an SRM on subject 1 and 2 and test it on subject 1 and 2 and output the average accuracy; train an SRM on subject 1,2,3 and test it on subject 1,2,3 and output the average accuracy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image class prediction<a id=\"image_pred\"></a>\n",
    "\n",
    "We have shown that cross-subject alignment (as measured, e.g., by time-segment matching) can be improved when SRM is trained on movie data and then tested on movie data. This shows that the shared space learned on movie data transfers well to *other movie data*. An important question is whether this shared space is only useful for movie data, or whether it generalizes to other kinds of data -- put another way, should we think of the shared space obtained from movie data as being a general purpose shared space that can be used to align other kinds of experiences (e.g., viewing static images), or is it a *specialized* space that can only be used to align movies? Here, we will address this question by training SRM on movie data and then applying the trained model to a different experiment (run in the same participants) where participants viewed static images from different categories. If we observe that SRM improves across-subject classification of image category (from the static-image-viewing experiment), this would suggest that the shared space learned by SRM is applicable more broadly to other visual experiments (not just to other movies). \n",
    "\n",
    "Note that the classification procedure here is basically the same as other image-classification analyses that we ran in previous notebooks. The main difference is that we are leaving out one subject (i.e., we are doing cross-subject classification) instead of doing within-subject classification. First, we will fit SRM (based on movie data), apply the transformation to the image-viewing data, and compute cross-subject classification using an SVM. Next, we will try the same cross-subject classification procedure without SRM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a leave-one-out cross validation with the subjects\n",
    "def image_class_prediction(image_data_shared, labels):\n",
    "    subjects = len(image_data_shared)\n",
    "    train_labels = np.tile(labels, subjects-1)\n",
    "    test_labels = labels\n",
    "    accuracy = np.zeros((subjects,))\n",
    "    for subject in range(subjects):\n",
    "        # Concatenate the subjects' data for training into one matrix\n",
    "        train_subjects = list(range(subjects))\n",
    "        train_subjects.remove(subject)\n",
    "        TRs = image_data_shared[0].shape[1]\n",
    "        train_data = np.zeros((image_data_shared[0].shape[0], len(train_labels)))\n",
    "        for train_subject in range(len(train_subjects)):\n",
    "            start_index = train_subject*TRs\n",
    "            end_index = start_index+TRs\n",
    "            train_data[:, start_index:end_index] = image_data_shared[train_subjects[train_subject]]\n",
    "\n",
    "        # Train a Nu-SVM classifier using scikit learn\n",
    "        classifier = NuSVC(nu=0.5, kernel='linear')\n",
    "        classifier = classifier.fit(train_data.T, train_labels)\n",
    "\n",
    "        # Predict on the test data\n",
    "        predicted_labels = classifier.predict(image_data_shared[subject].T)\n",
    "        accuracy[subject] = sum(predicted_labels == test_labels)/len(predicted_labels)\n",
    "        # Print accuracy\n",
    "        print(\"Accuracy for subj %d is: %0.4f\" % (subject, accuracy[subject] ))\n",
    "        \n",
    "    print(\"The average accuracy among all subjects is {0:f} +/- {1:f}\".format(np.mean(accuracy), np.std(accuracy)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**<a id=\"ex7\"></a> Fit an SRM with movie data and perform the image class prediction experiment on the transformed image data. Use features=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# Load movie data as training\n",
    "movie_data = np.load(os.path.join(raider_data_dir, 'movie.npy'))\n",
    "\n",
    "# Load image data and labels as testing\n",
    "image_data = np.load(os.path.join(raider_data_dir, 'image.npy'))\n",
    "labels = np.load(os.path.join(raider_data_dir, 'label.npy'))\n",
    "\n",
    "# convert to list\n",
    "train_data = []\n",
    "test_data = []\n",
    "for sub in range(num_subs):\n",
    "    train_data.append(movie_data[:,:,sub])  \n",
    "    test_data.append(image_data[:,:,sub])  \n",
    "del movie_data, image_data\n",
    "\n",
    "# Zscore training and testing data\n",
    "\n",
    "    \n",
    "# Create the SRM object\n",
    "\n",
    "\n",
    "# Fit the SRM data\n",
    "   \n",
    "\n",
    "# Transform the test data into the shared space using the individual weight matrices\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "\n",
    "\n",
    "# run the experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:**<a id=\"ex8\"></a> Run the image class prediction experiment on the raw (z-scored but not transformed by SRM) image data and compare the results with [Exercise 7](#ex7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SRM in comparison to other methods\n",
    "\n",
    "If you would like to see how SRM compares to other methods such as hyperalignment and canonical correlation analysis, refer to the supplementary section here: https://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:** <a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization.\n",
    "\n",
    "**Some ideas for novel contribution:** \n",
    "- Use cross-validation to select the best number of features in time segment matching or image class prediction experiments.\n",
    "- Explore the effect of different number of TRs in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 4/4/18  \n",
    "C. Chen provided initial code  \n",
    "H. Zhang major edits, added more exercises, added image class prediction section, filled in solutions, processed raider dataset, novel contribution ideas.  \n",
    "M. Kumar edited sections and added details to the time-segment matching function.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
